#!/usr/bin/env python3
"""
ì´ˆì›”ì  í•œêµ­ì–´ OCR ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
Ultimate Korean OCR Benchmark Script
"""

import sys
import os
import time
import json
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import pandas as pd
from datetime import datetime
import argparse
import logging

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.ultimate_korean_ocr import get_ultimate_ocr

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OCRBenchmark:
    """OCR ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.ocr = get_ultimate_ocr()
        self.results = []
        self.test_images = []
        
    def create_test_images(self) -> List[Tuple[str, np.ndarray, str]]:
        """ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±"""
        test_cases = []
        
        # 1. ì„ ëª…í•œ í…ìŠ¤íŠ¸
        img1 = np.ones((200, 600, 3), dtype=np.uint8) * 255
        text1 = "2024í•™ë…„ë„ 1í•™ê¸° í•™ìƒ í‰ê°€"
        cv2.putText(img1, "2024 1st Semester Student", (50, 100),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        test_cases.append(("clear_text", img1, text1))
        
        # 2. ì‘ì€ ê¸€ì”¨
        img2 = np.ones((150, 400, 3), dtype=np.uint8) * 255
        text2 = "ì‘ì€ ê¸€ì”¨ í…ŒìŠ¤íŠ¸"
        cv2.putText(img2, "Small Text Test", (50, 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)
        test_cases.append(("small_text", img2, text2))
        
        # 3. ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì´ë¯¸ì§€
        img3 = np.ones((200, 500, 3), dtype=np.uint8) * 255
        text3 = "ë…¸ì´ì¦ˆ í…ŒìŠ¤íŠ¸"
        cv2.putText(img3, "Noise Test", (50, 100),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        noise = np.random.normal(0, 25, img3.shape)
        img3 = np.clip(img3 + noise, 0, 255).astype(np.uint8)
        test_cases.append(("noisy_text", img3, text3))
        
        # 4. ë¸”ëŸ¬ ì²˜ë¦¬ëœ ì´ë¯¸ì§€
        img4 = np.ones((200, 500, 3), dtype=np.uint8) * 255
        text4 = "ë¸”ëŸ¬ í…ŒìŠ¤íŠ¸"
        cv2.putText(img4, "Blur Test", (50, 100),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        img4 = cv2.GaussianBlur(img4, (7, 7), 3)
        test_cases.append(("blurred_text", img4, text4))
        
        # 5. ë‚®ì€ ëŒ€ë¹„
        img5 = np.ones((200, 500, 3), dtype=np.uint8) * 200
        text5 = "ë‚®ì€ ëŒ€ë¹„"
        cv2.putText(img5, "Low Contrast", (50, 100),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (180, 180, 180), 2)
        test_cases.append(("low_contrast", img5, text5))
        
        # 6. íšŒì „ëœ í…ìŠ¤íŠ¸
        img6 = np.ones((300, 300, 3), dtype=np.uint8) * 255
        text6 = "íšŒì „ í…ìŠ¤íŠ¸"
        cv2.putText(img6, "Rotated", (100, 150),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        center = (150, 150)
        M = cv2.getRotationMatrix2D(center, 15, 1.0)
        img6 = cv2.warpAffine(img6, M, (300, 300))
        test_cases.append(("rotated_text", img6, text6))
        
        # 7. ë‹¤ì¤‘ ë¼ì¸
        img7 = np.ones((400, 600, 3), dtype=np.uint8) * 255
        lines = ["Line 1: Student Name", "Line 2: Grade Info", "Line 3: Subject Score"]
        text7 = " ".join(lines)
        y_pos = 100
        for line in lines:
            cv2.putText(img7, line, (50, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
            y_pos += 80
        test_cases.append(("multi_line", img7, text7))
        
        # 8. ë³µì¡í•œ ë°°ê²½
        img8 = np.random.randint(100, 200, (200, 500, 3), dtype=np.uint8)
        text8 = "ë³µì¡í•œ ë°°ê²½"
        cv2.putText(img8, "Complex BG", (50, 100),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 3)
        test_cases.append(("complex_bg", img8, text8))
        
        # 9. ë§¤ìš° í° ê¸€ì”¨
        img9 = np.ones((300, 800, 3), dtype=np.uint8) * 255
        text9 = "í° ê¸€ì”¨"
        cv2.putText(img9, "LARGE", (50, 200),
                   cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 0, 0), 5)
        test_cases.append(("large_text", img9, text9))
        
        # 10. ì™œê³¡ëœ ì´ë¯¸ì§€
        img10 = np.ones((200, 500, 3), dtype=np.uint8) * 255
        text10 = "ì™œê³¡ í…ŒìŠ¤íŠ¸"
        cv2.putText(img10, "Distorted", (50, 100),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        # ì›ê·¼ ë³€í™˜
        pts1 = np.float32([[0, 0], [500, 0], [0, 200], [500, 200]])
        pts2 = np.float32([[10, 10], [490, 20], [0, 200], [500, 190]])
        M = cv2.getPerspectiveTransform(pts1, pts2)
        img10 = cv2.warpPerspective(img10, M, (500, 200))
        test_cases.append(("distorted_text", img10, text10))
        
        return test_cases
    
    def run_benchmark(self, save_results: bool = True) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info("ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        test_cases = self.create_test_images()
        
        results = []
        total_time = 0
        
        # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì²˜ë¦¬
        for test_name, image, expected_text in tqdm(test_cases, desc="Processing"):
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            temp_path = f"/tmp/benchmark_{test_name}.jpg"
            cv2.imwrite(temp_path, image)
            
            # OCR ì²˜ë¦¬
            start_time = time.time()
            try:
                result = self.ocr.process_image(temp_path)
                processing_time = time.time() - start_time
                
                results.append({
                    'test_name': test_name,
                    'expected_text': expected_text,
                    'recognized_text': result.get('text', ''),
                    'confidence': result.get('confidence', 0),
                    'processing_time': processing_time,
                    'success': True,
                    'details': result.get('details', [])
                })
                
                total_time += processing_time
                
            except Exception as e:
                logger.error(f"Error processing {test_name}: {e}")
                results.append({
                    'test_name': test_name,
                    'expected_text': expected_text,
                    'recognized_text': '',
                    'confidence': 0,
                    'processing_time': time.time() - start_time,
                    'success': False,
                    'error': str(e)
                })
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            Path(temp_path).unlink(missing_ok=True)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        successful_results = [r for r in results if r['success']]
        
        metrics = {
            'total_tests': len(results),
            'successful': len(successful_results),
            'failed': len(results) - len(successful_results),
            'average_confidence': np.mean([r['confidence'] for r in successful_results]) if successful_results else 0,
            'min_confidence': min([r['confidence'] for r in successful_results]) if successful_results else 0,
            'max_confidence': max([r['confidence'] for r in successful_results]) if successful_results else 0,
            'average_time': np.mean([r['processing_time'] for r in results]),
            'total_time': total_time,
            'perfect_recognitions': len([r for r in successful_results if r['confidence'] >= 0.99]),
            'high_confidence': len([r for r in successful_results if r['confidence'] >= 0.90])
        }
        
        # OCR ì—”ì§„ ë©”íŠ¸ë¦­ ì¶”ê°€
        ocr_metrics = self.ocr.get_metrics()
        metrics['ocr_metrics'] = ocr_metrics
        
        # ê²°ê³¼ ì €ì¥
        if save_results:
            self.save_results(results, metrics)
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_results(results, metrics)
        
        # ê·¸ë˜í”„ ìƒì„±
        self.create_visualizations(results, metrics)
        
        return {
            'results': results,
            'metrics': metrics
        }
    
    def save_results(self, results: List[Dict], metrics: Dict):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_dir = Path("./benchmark_results")
        output_dir.mkdir(exist_ok=True)
        
        output_file = output_dir / f"benchmark_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'results': results,
                'metrics': metrics
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Results saved to {output_file}")
        
        # CSVë¡œë„ ì €ì¥
        df = pd.DataFrame(results)
        csv_file = output_dir / f"benchmark_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        logger.info(f"CSV saved to {csv_file}")
    
    def print_results(self, results: List[Dict], metrics: Dict):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        print(f"\nì´ í…ŒìŠ¤íŠ¸: {metrics['total_tests']}")
        print(f"ì„±ê³µ: {metrics['successful']} ({metrics['successful']/metrics['total_tests']*100:.1f}%)")
        print(f"ì‹¤íŒ¨: {metrics['failed']}")
        
        print(f"\ní‰ê·  ì‹ ë¢°ë„: {metrics['average_confidence']:.2%}")
        print(f"ìµœì†Œ ì‹ ë¢°ë„: {metrics['min_confidence']:.2%}")
        print(f"ìµœëŒ€ ì‹ ë¢°ë„: {metrics['max_confidence']:.2%}")
        
        print(f"\nì™„ë²½í•œ ì¸ì‹ (â‰¥99%): {metrics['perfect_recognitions']}")
        print(f"ë†’ì€ ì‹ ë¢°ë„ (â‰¥90%): {metrics['high_confidence']}")
        
        print(f"\ní‰ê·  ì²˜ë¦¬ ì‹œê°„: {metrics['average_time']:.3f}ì´ˆ")
        print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {metrics['total_time']:.3f}ì´ˆ")
        
        print("\nê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print("-"*60)
        
        for result in results:
            status = "âœ…" if result['success'] else "âŒ"
            confidence = result['confidence'] * 100
            time_ms = result['processing_time'] * 1000
            
            print(f"{status} {result['test_name']:15} | "
                  f"ì‹ ë¢°ë„: {confidence:5.1f}% | "
                  f"ì‹œê°„: {time_ms:6.1f}ms")
            
            if result['recognized_text']:
                print(f"   ì¸ì‹: {result['recognized_text'][:50]}")
    
    def create_visualizations(self, results: List[Dict], metrics: Dict):
        """ì‹œê°í™” ìƒì„±"""
        try:
            import matplotlib
            matplotlib.use('Agg')  # GUI ì—†ì´ ê·¸ë˜í”„ ìƒì„±
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # 1. ì‹ ë¢°ë„ ë¶„í¬
            confidences = [r['confidence'] for r in results if r['success']]
            axes[0, 0].hist(confidences, bins=20, edgecolor='black', alpha=0.7)
            axes[0, 0].set_xlabel('Confidence')
            axes[0, 0].set_ylabel('Count')
            axes[0, 0].set_title('Confidence Distribution')
            axes[0, 0].axvline(0.99, color='r', linestyle='--', label='99% threshold')
            axes[0, 0].legend()
            
            # 2. ì²˜ë¦¬ ì‹œê°„ ë¶„í¬
            times = [r['processing_time'] for r in results]
            axes[0, 1].bar(range(len(times)), times, color='skyblue')
            axes[0, 1].set_xlabel('Test Case')
            axes[0, 1].set_ylabel('Processing Time (s)')
            axes[0, 1].set_title('Processing Time per Test')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # 3. í…ŒìŠ¤íŠ¸ë³„ ì‹ ë¢°ë„
            test_names = [r['test_name'] for r in results]
            test_confidences = [r['confidence'] for r in results]
            
            axes[1, 0].barh(test_names, test_confidences, color='green')
            axes[1, 0].set_xlabel('Confidence')
            axes[1, 0].set_title('Confidence by Test Type')
            axes[1, 0].axvline(0.99, color='r', linestyle='--', alpha=0.5)
            
            # 4. ì„±ê³µ/ì‹¤íŒ¨ íŒŒì´ ì°¨íŠ¸
            success_counts = [metrics['successful'], metrics['failed']]
            labels = ['Success', 'Failed']
            colors = ['#90EE90', '#FFB6C1']
            
            axes[1, 1].pie(success_counts, labels=labels, colors=colors,
                          autopct='%1.1f%%', startangle=90)
            axes[1, 1].set_title('Success Rate')
            
            plt.tight_layout()
            
            # ê·¸ë˜í”„ ì €ì¥
            output_dir = Path("./benchmark_results")
            output_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_file = output_dir / f"benchmark_plot_{timestamp}.png"
            plt.savefig(plot_file, dpi=150, bbox_inches='tight')
            plt.close()
            
            logger.info(f"Visualizations saved to {plot_file}")
            
        except ImportError:
            logger.warning("Matplotlib not available. Skipping visualizations.")
        except Exception as e:
            logger.error(f"Error creating visualizations: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Ultimate Korean OCR Benchmark")
    parser.add_argument('--save', action='store_true', default=True,
                       help='Save benchmark results')
    parser.add_argument('--verbose', action='store_true',
                       help='Verbose output')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    
    print("="*60)
    print("ì´ˆì›”ì  í•œêµ­ì–´ OCR ë²¤ì¹˜ë§ˆí¬")
    print("Ultimate Korean OCR Benchmark")
    print("="*60)
    
    benchmark = OCRBenchmark()
    results = benchmark.run_benchmark(save_results=args.save)
    
    print("\në²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    
    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    score = (
        results['metrics']['average_confidence'] * 50 +  # ì‹ ë¢°ë„ 50%
        (results['metrics']['successful'] / results['metrics']['total_tests']) * 30 +  # ì„±ê³µë¥  30%
        (1 - min(results['metrics']['average_time'] / 2, 1)) * 20  # ì†ë„ 20%
    )
    
    print(f"\nìµœì¢… ì ìˆ˜: {score:.1f}/100")
    
    if score >= 95:
        print("ğŸ† ì™„ë²½í•œ ì„±ëŠ¥!")
    elif score >= 90:
        print("â­ ìš°ìˆ˜í•œ ì„±ëŠ¥!")
    elif score >= 80:
        print("âœ… ì¢‹ì€ ì„±ëŠ¥")
    else:
        print("âš ï¸ ê°œì„  í•„ìš”")
    
    return results


if __name__ == "__main__":
    main()
